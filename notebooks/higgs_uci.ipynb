{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sAu7KlnTm5-"
      },
      "source": [
        "# Analysis of Higgs Dataset, UCI way\n",
        "\n",
        "In questo notebook riproduco la rete neurale descritta nell'articolo:\n",
        "\n",
        "[Baldi, P., P. Sadowski, and D. Whiteson. “Searching for Exotic Particles in High-energy Physics with Deep Learning.” Nature Communications 5 (July 2, 2014)](https://www.nature.com/articles/ncomms5308.pdf) \n",
        "\n",
        "per lo studio delle capacità di un algoritmo ML nella discriminazione SB alla ricerca di nuove particelle (bosoni di Higgs pesanti).\n",
        "\n",
        "## Struttura notebook\n",
        "\n",
        "Il notebook avrà la seguente struttura:\n",
        "\n",
        "1. Download dei dati e preparazione dei set di training e di test;\n",
        "2. Definizione del modello di classificazione dei dati (in `tensorflow.keras`);\n",
        "3. Training del modello;\n",
        "4. Valutazione del modello su un set di test;\n",
        "5. Bellurie rafiche per mostrare i risultati.\n",
        "\n",
        "\n",
        "In questo notebook sfrutto alcune funzioni di uso generico per il download dei dati e la realizzazione di gif sviluppate per il progetto SpQR-Flow."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download della repository HitHub con SpQR-Flow\n",
        "\n",
        "! git clone https://github.com/MarcoRiggirello/SpQR-Flow.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGCHbAJBDNbI",
        "outputId": "f8422dee-5bab-423d-8740-72da9eed9343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'SpQR-Flow' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# per importare moduli in locale\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "py_file_location = \"/content/SpQR-Flow/SpQR-Flow\"\n",
        "sys.path.append(os.path.abspath(py_file_location))"
      ],
      "metadata": {
        "id": "7lBeHOHDDjXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uCLKLOhDJqM"
      },
      "outputs": [],
      "source": [
        "# Caricamento di alcune librerie\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import imageio\n",
        "from PIL import Image\n",
        "\n",
        "from download import download_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo990nEuDJqU"
      },
      "source": [
        "## Downloading data\n",
        "\n",
        "To download the dataset we use the function download_file in the \\texttt{download.py} module: it will check if the dataset already exists in the current directory or download it from the internet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "XBnSktkvDJqW"
      },
      "outputs": [],
      "source": [
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz'\n",
        "file_path = os.path.join('../model_test/download/HIGGS.csv.gz')\n",
        "download_file(url, file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPDBOrwBDJqZ"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQxj0pyGDJqb"
      },
      "outputs": [],
      "source": [
        "t0 = time.time()\n",
        "\n",
        "column_labels = ['label','lepton pT', 'lepton eta', 'lepton phi',\n",
        "                 'missing energy magnitude', 'missing energy phi',\n",
        "                 'jet 1 pt', 'jet 1 eta', 'jet 1 phi',\n",
        "                 'jet 1 b-tag', 'jet 2 pt', 'jet 2 eta',\n",
        "                 'jet 2 phi', 'jet 2 b-tag', 'jet 3 pt',\n",
        "                 'jet 3 eta', 'jet 3 phi', 'jet 3 b-tag',\n",
        "                 'jet 4 pt', 'jet 4 eta', 'jet 4 phi',\n",
        "                 'jet 4 b-tag', 'm_jj', 'm_jjj','m_lv',\n",
        "                 'm_jlv', 'm_bb', 'm_wbb', 'm_wwbb']\n",
        "data = pd.read_csv(file_path,\n",
        "                   header=0,\n",
        "                   names=column_labels,\n",
        "                   nrows=8192*512)\n",
        "t1 = time.time()\n",
        "print(f'time to read {data.ndim} events = {t1 - t2 :.0f} s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT0-rxzfDJqi"
      },
      "source": [
        "## Slicing data: train, test\n",
        "\n",
        "We slice the dataframe to obtain train and test samples.\n",
        "Data appear to have mixed labels already, so that a simple slicing is sufficient to obtain good sampling.\n",
        "\n",
        "There are 11 milions entries: following the article, the first 2M events are used are training data, the last 100k as tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n38SvmLJDJqk"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 8192\n",
        "DATASET_SIZE = BATCH_SIZE * 256\n",
        "TEST_SIZE = BATCH_SIZE * 16\n",
        "SEED = 42\n",
        "\n",
        "data_train = data[column_labels[1:22]].head(DATASET_SIZE).astype('float32')\n",
        "data_test = data[column_labels[1:22]].tail(TEST_SIZE).astype('float32')\n",
        "y_data = data[column_labels[0]].head(DATASET_SIZE).astype('int32')\n",
        "y_test = data[column_labels[0]].tail(TEST_SIZE).astype('int32')\n",
        "\n",
        "# Batching the data is not needed with keras\n",
        "#labels_batched = [y_data.sample(BATCH_SIZE, random_state=(SEED+i)) \n",
        "#                for i in range(int(DATASET_SIZE/BATCH_SIZE))]\n",
        "#features_batched = [data_train.sample(BATCH_SIZE, random_state=(SEED+i)) \n",
        "#                for i in range(int(DATASET_SIZE/BATCH_SIZE))]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Building the model\n",
        "\n",
        " The model to train is a Deep Forward Neural Network, with 5 hidden layers, 300 nodes each. Learning Rate = 0.05 and weight decay coefficient = \n",
        " $1 \\times 10^{-5}$.\n",
        " Regularization techniques are used, such as Dropout. Other techniques were not used in the article because of computing time expenses, ence we won't use them either."
      ],
      "metadata": {
        "id": "ZHC934EVmxBn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1qucOBbDJqr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input, layers, Model \n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "NODES = 300\n",
        "LR = 5e-2\n",
        "\n",
        "inputs = Input(shape=(21,))\n",
        "h_layer = layers.Dense(NODES, activation='relu')(inputs)\n",
        "h_layer = layers.Dense(NODES, activation='relu')(h_layer)\n",
        "h_layer = layers.Dropout(0.2)(h_layer)\n",
        "h_layer = layers.Dense(NODES, activation='relu')(h_layer)\n",
        "h_layer = layers.Dense(NODES, activation='relu')(h_layer)\n",
        "h_layer = layers.Dense(NODES, activation='relu')(h_layer)\n",
        "output = layers.Dense(1, activation='sigmoid')(h_layer)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs, name='uci')\n",
        "\n",
        "# Other options to the model\n",
        "model.compile(optimizer=Adam(learning_rate=LR),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "In this section we train the model."
      ],
      "metadata": {
        "id": "zHZqmzSLt1R_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(data_train, y_data,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=100,\n",
        "          verbose=2,\n",
        "          use_multiprocessing=True)"
      ],
      "metadata": {
        "id": "PjjBsCSut5nq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Higgs_analysis_UCI.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}