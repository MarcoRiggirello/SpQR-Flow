{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BdEMIwwDJp-"
   },
   "source": [
    "# SpQR application on a Higgs dataset\n",
    "\n",
    "\n",
    "In this notebook we train the SpQR model on the dataset HIGGS.csv from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/HIGGS).\n",
    "\n",
    "This notebook is divided essentially in five parts:\n",
    "\n",
    "    1. Downloading and preparing (slicing) the dataset\n",
    "    2. Plotting features (to get an initial idea)\n",
    "    3. Initializing the model\n",
    "    4. Fitting and evaluating performances\n",
    "    5. Plotting results\n",
    "\n",
    "\n",
    "## Package needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EGCHbAJBDNbI",
    "outputId": "48d55db9-e97a-4c9e-e45d-1a665732ce54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'diglm' already exists and is not an empty directory.\n",
      "cat: diglm/diglm/diglm.py: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/MarcoRiggirello/diglm.git\n",
    "! cat diglm/src/diglm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "7lBeHOHDDjXo"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "py_file_location = \"diglm/src\"\n",
    "sys.path.append(py_file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "6uCLKLOhDJqM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import imageio\n",
    "from PIL import Image\n",
    "\n",
    "from download import download_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yo990nEuDJqU"
   },
   "source": [
    "## Downloading data\n",
    "\n",
    "To download the dataset we use the function download_file in the \\texttt{download.py} module: it will check if the dataset already exists in the current directory or download it from the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "XBnSktkvDJqW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz'\n",
    "file_path = os.path.join('../model_test/download/HIGGS.csv.gz')\n",
    "download_file(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPDBOrwBDJqZ"
   },
   "source": [
    "## Plotting features of the dataset\n",
    "\n",
    "We use seaborn to display scatterplot matrices of the correlation between features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQxj0pyGDJqb"
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "column_labels = ['label','lepton pT', 'lepton eta', 'lepton phi',\n",
    "                 'missing energy magnitude', 'missing energy phi',\n",
    "                 'jet 1 pt', 'jet 1 eta', 'jet 1 phi',\n",
    "                 'jet 1 b-tag', 'jet 2 pt', 'jet 2 eta',\n",
    "                 'jet 2 phi', 'jet 2 b-tag', 'jet 3 pt',\n",
    "                 'jet 3 eta', 'jet 3 phi', 'jet 3 b-tag',\n",
    "                 'jet 4 pt', 'jet 4 eta', 'jet 4 phi',\n",
    "                 'jet 4 b-tag', 'm_jj', 'm_jjj','m_lv',\n",
    "                 'm_jlv', 'm_bb', 'm_wbb', 'm_wwbb']\n",
    "data = pd.read_csv(file_path,\n",
    "                   header=0,\n",
    "                   names=column_labels,\n",
    "                   nrows=8192*16)\n",
    "t1 = time.time()\n",
    "print(f'Time for reading all the data ~ {(t1-t0):.0f} s')\n",
    "\n",
    "# Creating a gif of  correlation scatter plots                                                                        \n",
    "tuple_var = [(1, 7), (8, 14), (15, 21), (22, 28)]\n",
    "grid_list = [sns.PairGrid(data.head(500),\n",
    "                          hue='label',\n",
    "                          vars=column_labels[init:end],\n",
    "                          corner=True)\n",
    "             for init, end in tuple_var]\n",
    "\n",
    "fig_list = []\n",
    "for i, grid in enumerate(grid_list):\n",
    "    grid.map_diag(sns.kdeplot)\n",
    "    grid.map_offdiag(sns.kdeplot)\n",
    "    grid.add_legend()\n",
    "    fig_name = f'sns_plot{i}.png'\n",
    "    grid.savefig(fig_name)\n",
    "    fig_list.append(imageio.imread(fig_name))\n",
    "\n",
    "imageio.mimsave('sns.gif',\n",
    "                fig_list,\n",
    "                duration = 5)\n",
    "im = Image.open('sns.gif')\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vT0-rxzfDJqi"
   },
   "source": [
    "## Slicing data: train, validation, test\n",
    "\n",
    "We slice the dataframe to obtain train and test samples.\n",
    "Data appear to have mixed labels already, so that a simple slicing is sufficient to obtain good sampling.\n",
    "\n",
    "There are 11 milions entries: the last 500k are used as tests, as in the [original paper](https://www.nature.com/articles/ncomms5308.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "n38SvmLJDJqk"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "TRAIN_SIZE = BATCH_SIZE * 16\n",
    "VAL_SIZE = BATCH_SIZE \n",
    "TEST_SIZE = BATCH_SIZE * 16\n",
    "\n",
    "ft_train = data[column_labels[22:]].head(TRAIN_SIZE).astype('float32')\n",
    "ft_val = data[column_labels[22:]].head(VAL_SIZE).astype('float32')\n",
    "ft_test = data[column_labels[22:]].tail(TEST_SIZE).astype('float32')\n",
    "\n",
    "l_train = data[column_labels[0]].head(TRAIN_SIZE).astype('int32')\n",
    "l_val = data[column_labels[0]].head(VAL_SIZE).astype('int32')\n",
    "l_test = data[column_labels[0]].tail(TEST_SIZE).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "_Pg-b4H_DJqw"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Recursive function to batch the data\n",
    "def batch(list_df, dataframe, batch_size):\n",
    "  if len(dataframe.index) <= batch_size:\n",
    "    list_df.append(tf.convert_to_tensor(dataframe))\n",
    "    return list_df\n",
    "  else:\n",
    "    list_df.append(tf.convert_to_tensor(dataframe.head(batch_size)))\n",
    "    dataframe = dataframe.iloc[batch_size:]\n",
    "    return batch(list_df, dataframe, batch_size) \n",
    "\n",
    "\n",
    "# Batching the samples: train, val and test\n",
    "l_train_batched_2 = batch([], l_train, BATCH_SIZE)\n",
    "ft_train_batched = batch([], ft_train, BATCH_SIZE)\n",
    "dict_train_batched = [{'features': ft_train_batched[i], 'labels': l_train_batched[i]}\n",
    "              for i in range(len(ft_train_batched))]\n",
    "\n",
    "l_val_batched = batch([], l_val, BATCH_SIZE)\n",
    "ft_val_batched = batch([], ft_val, BATCH_SIZE)\n",
    "dict_val_batched = [{'features': ft_val_batched[i], 'labels': l_val_batched[i]}\n",
    "              for i in range(len(ft_val_batched))]\n",
    "\n",
    "l_test_batched = batch([], l_test, BATCH_SIZE)\n",
    "ft_test_batched = batch([], ft_test, BATCH_SIZE)\n",
    "dict_test_batched = [{'features': ft_test_batched[i], 'labels': l_test_batched[i]}\n",
    "              for i in range(len(ft_test_batched))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rAUyM7oDJqq"
   },
   "source": [
    "## Building the DIGLM model\n",
    "\n",
    "In the following block, we build our Deeply Invertible Generalized Linear Model (DIGLM) algorithm.\n",
    "We will then be able to train the model on labelled data.\n",
    "\n",
    "The steps to create and train the model are:\n",
    "\n",
    "1. Create the desired transformed outputs from a simple distribution with the same dimensionality as the input data;\n",
    "2. Initialize the NeuralSplineFlow bijector;\n",
    "3. Initialize the DIGLM model;\n",
    "4. Create the training step functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "R1qucOBbDJqr"
   },
   "outputs": [],
   "source": [
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow_probability import glm as tfglm\n",
    "from tensorflow_probability import bijectors as tfb\n",
    "from tensorflow.keras import metrics # metrics to evaluate model performances\n",
    "\n",
    "from spqr import NeuralSplineFlow as NSF\n",
    "from Diglm import DIGLM\n",
    "\n",
    "\n",
    "neural_spline_flow = NSF(splits=4)\n",
    "glm = tfglm.Bernoulli()\n",
    "NUM_FEATURES = 7\n",
    "\n",
    "diglm = DIGLM(neural_spline_flow, glm, NUM_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEWZ7VSnDJqt"
   },
   "source": [
    "We define training functions to loop over train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "DC_QtV97DJqu"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(optimizer, target_sample):\n",
    "    \"\"\"\n",
    "    Train step function for the diglm model. Implements the basic steps for computing\n",
    "    and updating the trainable variables of the model. It also\n",
    "    calculates the loss on training and validation samples.\n",
    "\n",
    "    :param optimizer: Optimizer fro gradient minimization (or maximization).\n",
    "    :type optimizer: A keras.optimizers object\n",
    "    :param target_sample: dictonary of labels and features of data to train the model\n",
    "    :type target_sample: dict\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # calculating loss and its gradient of training data\n",
    "        loss = -tf.reduce_mean(diglm.weighted_log_prob(target_sample))\n",
    "    variables = tape.watched_variables()\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gk3ASq82DJqv"
   },
   "source": [
    "## Training\n",
    "\n",
    "We go on training the algorithm on train dataset. We also check for the loss on validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXKzJzJD6IuM"
   },
   "outputs": [],
   "source": [
    "accuracy = metrics.BinaryAccuracy()\n",
    "LR = 1e-3\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "loss = 0\n",
    "val_loss = 0\n",
    "history = []\n",
    "\n",
    "learning_rate = tf.Variable(LR, trainable=False)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "y_true = l_test_batched[0]\n",
    "y_pred, var_pred, grad_pred = diglm(ft_test_batched[0])\n",
    "accuracy.update_state(y_true,\n",
    "                      y_pred)\n",
    "print(f'accuracy before training = {accuracy.result().numpy():.3f}')\n",
    "\n",
    "accuracy.reset_state()\n",
    "for epoch in tf.range(NUM_EPOCHS):\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch n. {epoch+1}. Loss = {loss} \\t Val Loss = {val_loss}.\")\n",
    "    # Iteration on batch for training\n",
    "    for batch in dict_train_batched:\n",
    "        t_start = time.time() # to calculate the time of each iteration\n",
    "        loss = train_step(optimizer, batch)\n",
    "        val_loss = -tf.reduce_mean(diglm.weighted_log_prob(dict_val_batched[0]))\n",
    "        t_stop = time.time()\n",
    "\n",
    "        # Printing results\n",
    "        print(f'{(t_stop - t_start):.0f} s \\t loss = {loss} \\t val_loss {val_loss}')\n",
    "        history.append([loss, val_loss])\n",
    "\n",
    "        # Accuracy estimate\n",
    "        for j in range(len(l_test_batched)):\n",
    "          y_true = l_test_batched[j]\n",
    "          y_pred, var_pred, grad_pred = diglm(ft_test_batched[j])\n",
    "          accuracy.update_state(y_true,\n",
    "                                y_pred)\n",
    "        print(f'accuracy after training = {accuracy.result().numpy():.3f}')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "model_higgs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
